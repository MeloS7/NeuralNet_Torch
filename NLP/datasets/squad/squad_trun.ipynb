{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the longest context+answer in the train dataset\n",
    "def search_max_len(dataset):\n",
    "    max_len = 0\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        data = dataset[i]\n",
    "        context = data['context']\n",
    "        answer = data['answers']['text'][0]\n",
    "        marked_text = f\"{context} [SEP] {answer}\"\n",
    "        tokens = tokenizer(marked_text)['input_ids']\n",
    "        max_len = max(max_len, len(tokens))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_long_dataset(batch):\n",
    "    contexts = batch['context']\n",
    "    answers = [ans['text'][0] for ans in batch['answers']]\n",
    "    \n",
    "    # 构建批量的marked_text\n",
    "    marked_texts = [f\"{context} [SEP] {answer}\" for context, answer in zip(contexts, answers)]\n",
    "    \n",
    "    # 使用tokenizer批量处理marked_texts\n",
    "    tokenized_outputs = tokenizer(marked_texts)\n",
    "    input_ids = tokenized_outputs['input_ids']\n",
    "    \n",
    "    # 判断每个样本的长度是否不超过512\n",
    "    is_short_enough = [len(tokens) <= 512 for tokens in input_ids]\n",
    "    \n",
    "    return {\"is_short_enough\": is_short_enough}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   2%|▏         | 2000/87599 [00:04<03:14, 439.54 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 87599/87599 [02:59<00:00, 487.15 examples/s]\n",
      "Map: 100%|██████████| 10570/10570 [00:22<00:00, 468.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "new_train_dataset = train_dataset.map(remove_long_dataset, batched=True)\n",
    "new_val_dataset = val_dataset.map(remove_long_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 87599/87599 [00:01<00:00, 64914.45 examples/s]\n",
      "Filter: 100%|██████████| 10570/10570 [00:00<00:00, 55966.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "new_train_dataset = new_train_dataset.filter(lambda example: example['is_short_enough'])\n",
    "new_val_dataset = new_val_dataset.filter(lambda example: example['is_short_enough'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"is_short_enough\" key\n",
    "new_train_dataset = new_train_dataset.remove_columns(\"is_short_enough\")\n",
    "new_val_dataset = new_val_dataset.remove_columns(\"is_short_enough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 88/88 [00:04<00:00, 19.76ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.96s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 19.72ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/OneFly7/squad_combined_bert_512/commit/e7c3383524299f8ad1e9daf95ae26ab12ed26d49', commit_message='Upload dataset', commit_description='', oid='e7c3383524299f8ad1e9daf95ae26ab12ed26d49', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to Huggingface\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset_to_upload = DatasetDict({\n",
    "    'train': new_train_dataset,\n",
    "    'validation': new_val_dataset\n",
    "})\n",
    "\n",
    "dataset_to_upload.push_to_hub('squad_combined_bert_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
